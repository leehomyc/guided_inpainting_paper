% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{ruler}
\usepackage{color}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter
\def\ECCV18SubNumber{***}  % Insert your submission number here

\title{Image Inpainting using Block-wise Procedural Training with Annealed Adversarial Counterpart} % Replace with your title

\titlerunning{ECCV-18 submission ID \ECCV18SubNumber}

\authorrunning{ECCV-18 submission ID \ECCV18SubNumber}

\author{Anonymous ECCV submission}
\institute{Paper ID \ECCV18SubNumber}


\maketitle

\begin{abstract}
Recent advances in deep generative models have shown promising potential in predicting missing pixel values in an image using surrounding context. However, such models are either slow or fail to generate large hole contents. We present a new method for synthesizing high-quality photo-realistic inpaintings from incomplete images using conditional generative adversarial networks (conditional GANs). In particular, we introduce a block-wise training scheme, in conjunction with an annealed adversarial loss, to stabilize the training process of a very deep generative network. We also discuss the effectiveness of a novel perceptual similarity metric as an additional loss. Furthermore, we extend our framework to various inpainting scenarios, including object removal, image harmonization and guided inpainting. Extensive experiments and user-studies show that our method significantly outperforms existing methods in all these tasks.
\keywords{Image translation, image inpainting, image harmonization and image composition.}
\end{abstract}

\section{Introduction}
Image inpainting is the task to fill in the missing part of an image with visually plausible contents. It is one of the most typical operations of image editing~\cite{gatys2015texture} and low-level computer visions~\cite{komodakis2006image,hays2007scene}. The goal of image inpainting is to create semantically plausible contents with rich texture details, which can either be consistent with the original contents or is coherent with the known context such that the output image appears realistic. Other than image restoring and fixing, inpainting can also be used to remove unwanted objects, or in the case of guided inpainting, it can be used to composite with the contents from another image. In the latter scenario, we often need harmonization to adjust the appearance of the guidance image to make it compatible with the known context. Meanwhile, inpainting is needed to fill in the gaps between the two images.    

Traditional image inpainting methods mostly develop texture synthesis techniques to address the problem of hole-filling~\cite{bertalmio2000image,komodakis2006image,wexler2004space,barnes2009patchmatch,bertalmio2003simultaneous,wilczkowiak2005hole}. In~\cite{barnes2009patchmatch}, Barnes et al. proposes the Patch-Match algorithm which efficiently searches for the most similar patch to reconstruct the missing regions. Wilczkowiak et al.~\cite{wilczkowiak2005hole} takes further steps and detects desirable search regions to find better match patches. However, these methods only exploit the low-level signal of the known contexts to hallucinate missing regions and fall short of understanding and predicting high-level semantics. Furthermore, it is often challenging to capture the global structure of images by simply extending texture from surrounding regions. Another line of work for inpainting aims to fill in holes with content from another guidance image, by using composition and harmonization~\cite{hays2007scene,tsai2017deep}. The guidance image is often retrieved from a large database of images before it is pasted blended with the original image. Although these methods are able to propagate high-frequency details from the guidance image, they often introduce inconsistent regions and gaps which are easily detectable with human eyes.  

More recently, deep neural networks have exhibited excellent performance in various computer vision tasks, including texture synthesis and image completion. In particular, adversarial training becomes the de facto strategy to train an image inpainting model~\cite{pathak2016context,yeh2016semantic,li2017generative,yang2017high,iizuka2017globally}. Pathak et al.~\cite{pathak2016context} first proposes to train an encoder-decoder model to synthesize missing holes from surrounding pixels, using both the reconstruction loss and the adversarial loss. In~\cite{yeh2016semantic}, Yeh et al. addresses inpainting by using a pre-trained model to find the most similar encoding of the corrupted image. Yang et al.~\cite{yang2017high} proposes a multi-scale neural patch synthesis approach, which optimizes the hole contents such that its feature extracted from middle layers of a pre-trained CNN matches with the features of the surrounding context. The optimization greatly improves the inpainting quality and resolution at the cost of computational efficiency. Iizuka et al.~\cite{iizuka2017globally} instead proposes a pure feed-forward model trained with global and local GANs, and generates excellent results for small holes. However, DNN based methods have several limitations. First, they are either too slow due to optimization~\cite{yang2017high} or cannot generate sufficient high-frequency details, especially for large holes~\cite{iizuka2017globally}. Second, it is difficult to handle perceptual continuity, making it necessary to resort to post-processing (e.g. Poisson blending for~\cite{iizuka2017globally}) to smooth out the coalescing regions.

In practice, we found that directly training a very deep generative network to synthesize high-frequency details is difficult. Most often we fail to stabilize the training process and the results are either overly smooth or containing significant noise and artifacts. To overcome this limitation, we discuss a new approach that produces high-quality inpainting results for various inpainting tasks, refer to as \textbf{B}lock-wise \textbf{T}rained \textbf{G}enerative \textbf{M}odel for \textbf{I}npainting (BTGMI). More specifically, we decompose the generator into a ResNet head followed by multiple refinement residual blocks. For the first phase, we train the \textbf{ResNet head} for inpainting until it converges. Then we add residual blocks one at a time. Each time we train with an additional block, we use skip connections to initialize with the trained network and gradually increase the weight of the new block. This introduces the new block gradually, forcing it to refine from the previous results and learn to generate richer details. In addition, we observe that it is essential to steadily reduce the weight of the generator adversarial loss during block-wise refinement. We refer to this training scheme as \textbf{A}dversarial \textbf{L}oss \textbf{A}nnealing (ALA).  Intuitively, AAL is helpful as the adversarial loss usually dominates in the end phase of training and the generator will falsely create noise patterns to foul the discriminator. Finally, Zhang et al.~\cite{zhang2018unreasonable} shows that the perceptual similarity, measured by internal activations of networks trained for high-level classification tasks, corresponds to human perceptual judgment far better than commonly used metrics such as the Euclidean distance. Inspired by this finding, we introduce a novel \textbf{P}atch \textbf{P}erceptual \textbf{L}oss (PPL), which penalize the perceptual difference between the inpainted patch and the original patch. Different from the perceptual losses ,used in style transfer~\cite{johnson2016perceptual,gatys2016image} and image synthesis~\cite{dosovitskiy2016generating,chen2017photographic}, we compute the feature disparity across all layers. In our experiment, we found that PPL works better than the reconstruction loss and the general perceptual loss. 

To evaluate the proposed inpainting approach, we conduct extensive experiments on different datasets. We also show that our model, although being designed for inpainting, can be used for general image translation tasks including image harmonization and composition. This enables us to jointly train inpainting with those tasks and makes it suitable for a wide range of inpainting scenarios such as object removal and guided inpainting. As shown by visual results and user-study, our network already outperforms state-of-the-art inpainting and harmonization methods without using block-wise refinement. By leveraging block-wise training, it can further add high-frequency details and eliminate the perceptual discontinuity. Finally, we demonstrate our approach is both effective and simple to use in multiple real use cases.

In summary, in this paper we present:
\begin{enumerate}
\item The Block-wise Trained Generative Model for Inpainting (BTGMI) as a novel, end-to-end model that generates state-of-the-art image inpainting and image composition results.  
\item The Adversarial Loss Annealing (ALA) and the Patch Perceptual Loss (PPL) as a novel training scheme and training loss that improve the quality of inpainting. 
\item An easy-to-use user interface for real use cases of distractor removal and guided inpainting, where the users could conveniently select objects from either the original image (for removal) or from the guided image (for composition).
\end{enumerate}


\section{Related Work}

\noindent\textbf{Deep image generation and manipulation} Generative Adversarial Network (GAN)~\cite{goodfellow2014generative} uses a mini-max two-player game to alternatively train a generator and a discriminator, and has shown impressive capacity to generate natural and high-quality images. However, for the vanilla GANs, the training instability makes it hard to scale to higher resolution images. Several techniques have been proposed to stabilize the training process, including Laplacian pyramid GAN~\cite{denton2015deep}, DCGAN~\cite{radford2015unsupervised}, energy-based GAN~\cite{zhao2016energy}, Wasserstein GAN (WGAN)~\cite{arjovsky2017wasserstein}, WGAN-GP~\cite{gulrajani2017improved} and the Progressive GAN~\cite{karras2017progressive}. Both BTGMI and Progressive GAN gradually increase the depth of the network during training. While Progressive GAN addresses the image synthesis problem, our architecture poses as an ideal candidate for image translation tasks. A major model difference between BTGMI and Progressive GAN is that, rather than bringing in convolutional layers at the end of the network, we progressively insert residual blocks before the upsampling layers. 

Adversarial training, as a general idea for DNN based methods, has been widely applied to various research fields, especially many image editing tasks such as image super-resolution~\cite{kim2016accurate,dong2014learning,ledig2016photo}, image-to-image translation~\cite{isola2016image,zhu2017unpaired}, image inpainting and image harmonization. Recently,~\cite{wang2017high} proposes the Pix2Pix HD model for high-resolution image synthesis using conditional GANs, which produces very high-quality simulated images from semantic maps, human sketches, etc. Our ResNet head simplifies the Pix2Pix HD model and also improves the synthesis quality by re-designing its losses. For the image inpainting task, many DNN based approaches achieve good performance by different network topology and training procedure ~\cite{pathak2016context,yang2017high,yeh2016semantic,iizuka2017globally}. Image harmonization, on the other hand, aims to adjust the appearances of the foreground and background regions such that they are compatible and the composition is realistic. In~\cite{zhu2015learning}, Zhu et al. trains a CNN model to measure how realistic of a composite image is, and uses this metric to adjust and optimize the appearance of the foreground region. Tsai et al.~\cite{tsai2017deep} also proposes a DNN based method by training a deep CNN to learn and predict the context and semantic information of composite images. However, the limitation of image harmonization alone is that low-level appearance or color adjustments are often inadequate to make the composition realistic. In contrast, our approach of guided inpainting not only adjusts the appearance of the guidance patch, but also synthesizes new contents to fill in the gaps and smoothes the transition between the foreground and the background.

\noindent\textbf{Non-neural image inpainting and harmonization} Traditional image completion algorithms can be either diffusion-based~\cite{bertalmio2000image,elad2005simultaneous} or patch-based~\cite{bertalmio2003simultaneous,barnes2009patchmatch}. Diffusion-based methods usually cannot synthesize plausible contents for large holes or textures, due to the fact that it only propagates low-level features. Patch-based methods, however, largely rely on the assumption that the desired patches exist in the database. For harmonization, traditional methods usually apply color and tone matching, by matching global statistics~\cite{reinhard2001color} or multi-scale statistics~\cite{sunkavalli2010multi}, extracting gradient domain information~\cite{perez2003poisson,tao2010error}, or utilizing semantic clues~\cite{tsai2016sky}.~\cite{johnson2011cg2real} further develops a data-driven method, which searches and retrieves multiple real images with similar structural layouts and use them to transfer the appearances. A complete comparison with non-neural inpainting and harmonization algorithms is beyond the scope of our paper.

\section{Our Method}

In this section, we describe our model and several training schemes. First, we illustrate the details of our basic components: the Generator head and the training losses in Sec.~\ref{sec:resnet_head}. Then, we characterize our block-wise procedural training scheme together with the adversarial loss weight annealing in Sec. ?. Finally, we present our implementation and training details ?. 

\subsection{The Generator Head}
\label{sec:resnet_head}

Our generator head is a conditional GAN network~\cite{mirza2014conditional}, which takes an incomplete image as the input, and outputs a complete image. Conditional GANs for image inpainting usually consist of a generator $G$ and a discriminator $D$. The generator $G$ learns to predict the hole contents and restore the complete image, while the discriminator $D$ learns to distinguish real images from the generated ones. The model is trained in a self-supervised manner via the following minimax game:
\[
\min\limits_G \max\limits_D E_{(s,x)}[\log D(s,x)] + E_s[\log (1-D(s,G(s)))],
\]
where $s$ and $x$ are the incomplete image and the original image respectively, and $G(s)$ is the generator prediction given the input $s$. Note that if $G(s)$ predicts an entire image as output, we only keep the hole contents and concatenate with the known context of $s$.

Previous research experimented with different architecture of $G$, most notably the U-Net style generator of~\cite{pathak2016context} and the FCN style generator of~\cite{iizuka2017globally}. ~\cite{iizuka2017globally} shows that FCN style inpainting network produces less blurred results than U-Net, mainly because instead of using the fully connected layer as a bottleneck, it only uses fully convolutional layers which avoids significant resolution reduction or information loss.

Similar to~\cite{iizuka2017globally}, our generator head is based on FCN and leverage the properties of convolutional neural networks, including translation invariance and parameter sharing. Nevertheless, a major limitation of FCN is the constraint of the receptive field size, since the convolution layers are locally connected, making pixels far away from the hole carry no influence on the predicted hole content. We rely on several strategies to alleviate such drawback. First, like~\cite{iizuka2017globally}, we use a down-sampling front end to reduce the feature size, followed by multiple ResNet blocks, as well as an up-sampling back end to restore the full dimension. By downsampling, we increase the receptive field of the ResNet blocks. Second, we stack multiple ResNet blocks to further enlarge the receptive field. Finally, we adopt the dilated convolutional layers~\cite{yu2015multi} in all ResNet blocks, with the dilation factor set to 2. Dilated convolutions use spaced kernels, making it compute each output value with a larger input coverage, without increasing the number of parameters and computational power. Overall, as context is critical for realism, we observed that the receptive size poses as an important role for image inpainting, which also differentiates it from other image translation tasks. 

More specifically, the down-sampling front-end consists of three convolutional layers each with stride 2, and the intermediate residual blocks contain 9 blocks stacked together, and the up-sampling back-end consists of three transposed convolution of stride 2. Each convolutional layer is followed by batch normalization (BN) and ReLu as the activation layer, except for the last layer which outputs the image. For down-sampling and up-sampling, an alternative would be to use interpolated convolution to reduce the checkerboard effect, as suggested by~\cite{odena2016deconvolution}. Interpolated convolution uses a dimension-preserving convolution layer of stride 1, followed by max pooling or bilinear up-sampling. However, we observed that using interpolated convolution creates overly smooth effects. A detailed ablation study is presented in Sec.~\ref{sec:results}.

The detailed architecture of our generator head is illustrated in Fig..

\subsection{The Training Losses}
Traditional approaches for inpainting has used the reconstruction loss, the adversarial loss, and the global local GAN loss. The reconstruction loss can be $l2$ or $l1$, which forces the prediction to be consistent with the original image, and the adversarial loss makes the results looks more realistic. Unlike previous works, we proposed the Patch Perceptual loss for reconstruction, and the improved multi-scale discriminator as discriminator loss. Table shows the different training losses used by different previous approaches.

\begin{table}[h!]
\begin{center}

\resizebox{1\textwidth}{!}{%
{\tiny
  \begin{tabular}{ l  c  c }
    \hline
    \textbf{Method} & \textbf{Reconstruction Loss} &  \textbf{Realism Loss} \\ \hline
    \emph{Context Encoder~\cite{pathak2016context}} & L2 \% & Global Adversarial\\ \hline
    \emph{Global local} & L2 & Global adversarial loss and local adversarial loss \\ \hline
    \emph{Our Approach} & PPL\% & improved multi-scale discriminator loss \\ \hline
    \hline
  \end{tabular}}
  }
  \end{center}
  \caption{Numerical comparison on 200 test images of ImageNet.}
  \vspace{-10pt}
  \label{table:numerical}
\end{table}

\noindent\textbf{Patch Perceptual Loss} The problem with L2 loss. Perceptual losses has been used in style transfer and image synthesis. The perceptual loss, uses a fixed and pretrained network to compute the feature of two images, and compute the distance of the features as the metric. Recently, [] gives thorough analysis and show that a new perceptual metric is more similar to human perception to measure the difference of two images, than L2, L1, SSIM, or PSNR. In particular, perceptual loss, will avoid gice blurry, or noisy patterns which penalize a lot. We propose to use the similar ways to compute perceptual metric and use it as a perceptual loss for training. To compute distance between two patches, given a network F, we first compute deep embeddings, normalize the activations in the channel dimension, scale each channel by vector w, and take the l2 distance. We then average across spatial dimension and across all layers. Furthermore, to ensure not just the hole contents are similar, we want to ensure the boundary and the connecting parts also look similar to. Hence we compute PPL at two scales, one just containing the hole, the other zoom in and overlaps a bit with the original image. We use AlexNet as the de facto network to measure the metric.

\noindent\textbf{Improved Multi-scale discriminator Loss} Discerning whether an image is real or has been completed. High-resolution image synthesis poses a great challenge to the GAN discriminator design. To differentiate high-resolution real and synthesized images, the discriminator needs to have a large receptive field. This would reuquire either a deeper network or larger convolutional kernels. As both choices lead to an incraeased network capacity, overfitting would become more of a concern. [] Proposes to use multiple GAN discriminators at different image scales, which either has a more global view which makes the image more globally consistent, or has a local GAN produce finer details. In addition, we use a convolutional "PatchGAN" classifier, which only penalizes strcture at the scale of image patches, it is better to capture better local style statistics. Which is also proposed in. However, directly using PatchGAN is probablematic in our setting, as even for a fake image, the patches outside the hole are from the original images, which should be real. Only the patches overlap with the hole should considered as fake. Therefore, when compute the losses, we do not output a single dimensional vector of True or False, we need to be more delicate and give different labels to different patches. This is why we call it improved multi-scale discriminator loss. In summary, we reformulate the patchGAN as.
\[
\min\max L_{GAN}(G,D)
\]
Comparing with Global GAN, our improved multiscale discriminator cares about the local style statistics, and makes local patches look more realistic. Comparing with local GAN, which takes the local patch around the hole as input to the local discriminator, our loss is more flexible as it is able to handle holes of different shape, location and sizes. Our multiscale also gives better results at different scales, and is critical in obtaining semantically and locally coherent image completion results.

Our full objective combines both losses as:

where $\lambda$ controls the importance of the two terms.

\subsection{Procedural Training with Adversarial Loss Annealing}

\subsection{Multiple Output.}

Our algorithm can be summarized in Alg.
\section{Results}
\label{sec:results}

\clearpage

\bibliographystyle{splncs}
\bibliography{egbib}
\end{document}
