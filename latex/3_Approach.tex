%!TEX root = guided_inpainting_paper.\section{Our Method}
\section{Our Method}

In this section, we describe our model and several training schemes. First, we illustrate the details of our basic components: the Generator head and the training losses in Sec.~\ref{sec:resnet_head}. Then, we characterize our block-wise procedural training scheme together with the adversarial loss weight annealing in Sec. ?. Finally, we present our implementation and training details ?. 

\subsection{The Generator Head}
\label{sec:resnet_head}

Our generator head is a conditional GAN network~\cite{mirza2014conditional}, which takes an incomplete image as the input, and outputs a complete image. Conditional GANs for image inpainting usually consist of a generator $G$ and a discriminator $D$. The generator $G$ learns to predict the hole contents and restore the complete image, while the discriminator $D$ learns to distinguish real images from the generated ones. The model is trained in a self-supervised manner via the following minimax game:
\[
\min\limits_G \max\limits_D E_{(s,x)}[\log D(s,x)] + E_s[\log (1-D(s,G(s)))],
\]
where $s$ and $x$ are the incomplete image and the original image respectively, and $G(s)$ is the generator prediction given the input $s$. Note that if $G(s)$ predicts an entire image as output, we only keep the hole contents and concatenate with the known context of $s$.

Previous research experimented with different architecture of $G$, most notably the U-Net style generator of~\cite{pathak2016context} and the FCN style generator of~\cite{iizuka2017globally}. ~\cite{iizuka2017globally} shows that FCN style inpainting network produces less blurred results than U-Net, mainly because instead of using the fully connected layer as a bottleneck, it only uses fully convolutional layers which avoids significant resolution reduction or information loss.

Similar to~\cite{iizuka2017globally}, our generator head is based on FCN and leverage the properties of convolutional neural networks, including translation invariance and parameter sharing. Nevertheless, a major limitation of FCN is the constraint of the receptive field size, since the convolution layers are locally connected, making pixels far away from the hole carry no influence on the predicted hole content. We rely on several strategies to alleviate such drawback. First, like~\cite{iizuka2017globally}, we use a down-sampling front end to reduce the feature size, followed by multiple ResNet blocks, as well as an up-sampling back end to restore the full dimension. By downsampling, we increase the receptive field of the ResNet blocks. Second, we stack multiple ResNet blocks to further enlarge the receptive field. Finally, we adopt the dilated convolutional layers~\cite{yu2015multi} in all ResNet blocks, with the dilation factor set to 2. Dilated convolutions use spaced kernels, making it compute each output value with a larger input coverage, without increasing the number of parameters and computational power. Overall, as context is critical for realism, we observed that the receptive size poses as an important role for image inpainting, which also differentiates it from other image translation tasks. 

More specifically, the down-sampling front-end consists of three convolutional layers each with stride 2, and the intermediate residual blocks contain 9 blocks stacked together, and the up-sampling back-end consists of three transposed convolution of stride 2. Each convolutional layer is followed by batch normalization (BN) and ReLu as the activation layer, except for the last layer which outputs the image. For down-sampling and up-sampling, an alternative would be to use interpolated convolution to reduce the checkerboard effect, as suggested by~\cite{odena2016deconvolution}. Interpolated convolution uses a dimension-preserving convolution layer of stride 1, followed by max pooling or bilinear up-sampling. However, we observed that using interpolated convolution creates overly smooth effects. A detailed ablation study is presented in Sec.~\ref{sec:results}.

The detailed architecture of our generator head is illustrated in Fig..

\subsection{The Training Losses}
Traditional approaches for inpainting has used the reconstruction loss, the adversarial loss, and the global local GAN loss. The reconstruction loss can be $l2$ or $l1$, which forces the prediction to be consistent with the original image, and the adversarial loss makes the results looks more realistic. Unlike previous works, we proposed the Patch Perceptual loss for reconstruction, and the improved multi-scale discriminator as discriminator loss. Table shows the different training losses used by different previous approaches.

\begin{table}[h!]
\begin{center}

\resizebox{1\textwidth}{!}{%
{\tiny
  \begin{tabular}{ l  c  c }
    \hline
    \textbf{Method} & \textbf{Reconstruction Loss} &  \textbf{Realism Loss} \\ \hline
    \emph{Context Encoder~\cite{pathak2016context}} & L2 \% & Global Adversarial\\ \hline
    \emph{Global local} & L2 & Global adversarial loss and local adversarial loss \\ \hline
    \emph{Our Approach} & PPL\% & improved multi-scale discriminator loss \\ \hline
    \hline
  \end{tabular}}
  }
  \end{center}
  \caption{Numerical comparison on 200 test images of ImageNet.}
  \vspace{-10pt}
  \label{table:numerical}
\end{table}

\noindent\textbf{Patch Perceptual Loss} The problem with L2 loss. Perceptual losses has been used in style transfer and image synthesis. The perceptual loss, uses a fixed and pretrained network to compute the feature of two images, and compute the distance of the features as the metric. Recently, [] gives thorough analysis and show that a new perceptual metric is more similar to human perception to measure the difference of two images, than L2, L1, SSIM, or PSNR. In particular, perceptual loss, will avoid gice blurry, or noisy patterns which penalize a lot. We propose to use the similar ways to compute perceptual metric and use it as a perceptual loss for training. To compute distance between two patches, given a network F, we first compute deep embeddings, normalize the activations in the channel dimension, scale each channel by vector w, and take the l2 distance. We then average across spatial dimension and across all layers. Furthermore, to ensure not just the hole contents are similar, we want to ensure the boundary and the connecting parts also look similar to. Hence we compute PPL at two scales, one just containing the hole, the other zoom in and overlaps a bit with the original image. We use AlexNet as the de facto network to measure the metric.

\noindent\textbf{Improved Multi-scale discriminator Loss} Discerning whether an image is real or has been completed. High-resolution image synthesis poses a great challenge to the GAN discriminator design. To differentiate high-resolution real and synthesized images, the discriminator needs to have a large receptive field. This would reuquire either a deeper network or larger convolutional kernels. As both choices lead to an incraeased network capacity, overfitting would become more of a concern. [] Proposes to use multiple GAN discriminators at different image scales, which either has a more global view which makes the image more globally consistent, or has a local GAN produce finer details. In addition, we use a convolutional "PatchGAN" classifier, which only penalizes strcture at the scale of image patches, it is better to capture better local style statistics. Which is also proposed in. However, directly using PatchGAN is probablematic in our setting, as even for a fake image, the patches outside the hole are from the original images, which should be real. Only the patches overlap with the hole should considered as fake. Therefore, when compute the losses, we do not output a single dimensional vector of True or False, we need to be more delicate and give different labels to different patches. This is why we call it improved multi-scale discriminator loss. In summary, we reformulate the patchGAN as.
\[
\min\max L_{GAN}(G,D)
\]
Comparing with Global GAN, our improved multiscale discriminator cares about the local style statistics, and makes local patches look more realistic. Comparing with local GAN, which takes the local patch around the hole as input to the local discriminator, our loss is more flexible as it is able to handle holes of different shape, location and sizes. Our multiscale also gives better results at different scales, and is critical in obtaining semantically and locally coherent image completion results.

Our full objective combines both losses as:

where $\lambda$ controls the importance of the two terms.

\subsection{Procedural Training with Adversarial Loss Annealing}

\subsection{Multiple Output.}

Our algorithm can be summarized in Alg.
