%!TEX root = guided_inpainting_paper.tex
\section{Our Method}

In this section, we describe our model and several training schemes. First, we illustrate the details of our basic components: the Generator head and the training losses in Sec.~\ref{sec:resnet_head}. Then, we characterize our block-wise procedural training scheme together with the adversarial loss weight annealing in Sec.~\ref{sec:procedural}. Finally, we summarize our implementation and training details. 

\subsection{The Generator Head}
\label{sec:resnet_head}

Our generator head is a conditional GAN network~\cite{mirza2014conditional}, which takes an incomplete image as the input, and outputs a complete image. Conditional GANs for image inpainting usually consist of a generator $G$ and a discriminator $D$. The generator $G$ learns to predict the hole contents and restore the complete image, while the discriminator $D$ learns to distinguish real images from the generated ones. The model is trained in a self-supervised manner via the following minimax game:
\begin{eqnarray}
\min\limits_G \max\limits_D E_{(s,x)}[\log D(s,x)] + E_s[\log (1-D(s,G(s)))],
\end{eqnarray}
where $s$ and $x$ are the incomplete image and the original image respectively, and $G(s)$ is the generator prediction given the input $s$. Note that if $G(s)$ predicts an entire image as output, we only keep the hole contents and concatenate with the known context of $s$.

Previous research experimented with different architecture of $G$, most notably the U-Net style generator of~\cite{pathak2016context} and the FCN style generator of~\cite{iizuka2017globally}. ~\cite{iizuka2017globally} shows that FCN style inpainting network produces less blurred results than U-Net, mainly because instead of using the fully connected layer as a bottleneck, it only uses fully convolutional layers which avoids significant resolution reduction or information loss.

Similar to~\cite{iizuka2017globally}, our generator head is based on FCN and leverage the properties of convolutional neural networks, including translation invariance and parameter sharing. Nevertheless, a major limitation of FCN is the constraint of the receptive field size, since the convolution layers are locally connected, making pixels far away from the hole carry no influence on the predicted hole content. We rely on several strategies to alleviate such drawback. First, like~\cite{iizuka2017globally}, we use a down-sampling front end to reduce the feature size, followed by multiple ResNet blocks, as well as an up-sampling back end to restore the full dimension. By downsampling, we increase the receptive field of the ResNet blocks. Second, we stack multiple ResNet blocks to further enlarge the receptive field. Finally, we adopt the dilated convolutional layers~\cite{yu2015multi} in all ResNet blocks, with the dilation factor set to 2. Dilated convolutions use spaced kernels, making it compute each output value with a larger input coverage, without increasing the number of parameters and computational power. Overall, as context is critical for realism, we observed that the receptive size poses as an important role for image inpainting, which also differentiates it from other image translation tasks. 

More specifically, the down-sampling front-end consists of three convolutional layers each with stride 2, and the intermediate residual blocks contain 9 blocks stacked together, and the up-sampling back-end consists of three transposed convolution of stride 2. Each convolutional layer is followed by batch normalization (BN) and ReLu as the activation layer, except for the last layer which outputs the image. For down-sampling and up-sampling, an alternative would be to use interpolated convolution to reduce the checkerboard effect, as suggested by~\cite{odena2016deconvolution}. Interpolated convolution uses a dimension-preserving convolution layer of stride 1, followed by max pooling or bilinear up-sampling. However, we observed that using interpolated convolution creates overly smooth effects. A detailed ablation study is presented in Sec.~\ref{sec:results}.

The detailed architecture of our generator head is illustrated in Fig..

\subsection{The Training Losses}
Different losses have been used to train an inpainting network. These losses can be cast into two categories. The first category, which we refer to as \textit{similarity loss}, is used to measure the similarity between the output and the original image. The second category, which we refer to as the \textit{realism loss}, is used to measure how realistic-looking the output image is. We summarize the losses used in different approaches in Table~\ref{table:losses}. 

\begin{table}[h!]
\begin{center}

\resizebox{1\textwidth}{!}{%
{\tiny
  \begin{tabular}{ l  c  c }
    \hline
    \textbf{Method} & \textbf{Similarity Loss} &  \textbf{Realism Loss} \\ \hline
    \emph{Context Encoder~\cite{pathak2016context}} & $\ell_2$ & Global Adversarial Loss\\ \hline
    \emph{Global Local GAN~\cite{iizuka2017globally}} & $\ell_2$ & Global and Local Adversarial Loss \\ \hline
    \emph{Our Approach} & Patch Perceptual Loss (PPL) & Improved Multi-Scale Adversarial Loss \\ \hline
    \hline
  \end{tabular}}
  }
  \end{center}
  \caption{Comparison of training losses in different methods.}
  \vspace{-10pt}
  \label{table:losses}
\end{table}

\noindent\textbf{Patch Perceptual Loss} As shown in Table~\ref{table:losses}, using $\ell_2$ loss for reconstruction and measure the disparity between the output and the original image has been a default choice of previous inpainting methods. However, it is known that $\ell_2$ loss does not correspond well to human perception of visual similarity~\cite{zhang2018unreasonable}. This is because $\ell_2$ losses wrongly assumes each output pixel is conditionally independent of all others. A well-known issue, for example, is that blurring an image leads to small changes in terms of Euclidean distance but causes significant perceptual difference. Recent research suggests that a better metric for perceptual similarity is the internal activations of deep convolutional networks, usually trained on a high-level image classification task. Such loss is called ``perceptual loss'', and is used in various tasks such as neural style transfer~\cite{gatys2016image}, image super-resolution~\cite{johnson2016perceptual}, and conditional image synthesis~\cite{dosovitskiy2016generating,chen2017photographic}.

Based on this observation, we propose a new ``patch perceptual loss'' as the substitute of the $\ell_2$ losses. Traditional perceptual loss typically uses VGG-Net, and computes the $\ell_2$ distance of the activations on a few feature layers. Recently,~\cite{zhang2016stackgan} specifically trained a patch perceptual network to measure the perceptual differences between two image patches based on AlexNet, making it an ideal candidate for our task. The patch perceptual network computes the activations across all feature layers and sums up the $\ell_2$ distances scaled by learned weights at each layer. Furthermore, to take into account both the local view and the global view of perceptual similarity, we compute PPL at two scales. Local PPL considers the local hole patch, while the global PPL slightly zooms out to cover a larger contextual area. More formally, our PPL is defined as:
\begin{eqnarray}
\sum\limits_{k=1,2}PPL_k(G(s)_p, x_p) = \sum\limits_{k=1,2}\sum\limits_l\frac{1}{H_lW_l}\sum\limits_{h,w}\parallel w_l^T\odot(\hat{F}(x_p)^l_{hw}-\hat{F}(G(s))^k_{hw})\parallel^2_2.
\end{eqnarray}

Here $k$ refers to the patch scale. $p$ is the patch covering the hole. $\hat{F}$ is the AlexNet and $l$ is the feature layer. Sec.~\ref{sec:results} shows that PPL gives better inpainting quality than both $\ell_2$ and VGG-based perceptual losses. 

\noindent\textbf{Multi-Scale Patch-wise Adversarial Loss} Adversarial losses are given by trained discriminators to discern whether an image is real or fake. The global adversarial loss of~\cite{pathak2016context} takes the entire image as input and outputs a single real/fake prediction, which does not consider the realism of local patches, especially the holes. The additional local adversarial loss of~\cite{iizuka2017globally} adds another discriminator specifically for the hole, but it requires the hole to be fixed shape and size during training to fit the discriminator. To consider both the global view and the local view, we propose to use discriminators at three scales of image resolutions. The discriminator at each scale is identical, only the input is a scaled version of the \textit{entire image}. Furthermore, we use PatchGAN discriminator at each scale, which uses convolutional discriminator to output a vector of predictions, each value corresponds to an image patch. In this way, the discriminators are trained to classify global and local image patches across the image, which enables us to use random holes and shapes during training. However, directly using PatchGAN is problematic in our case, as for the output restored image, only the patches overlapping with the hole area should be considered fake patches. Therefore when computing the discriminator loss of the restored image, instead of forcing the entire output to be fake, only the patches overlapping with the holes are labeled as fake. More formally, our Patch-wise Adversarial Loss is defined as: 
\begin{eqnarray}
\min\limits_G\max\limits_{D_1, D_2, D_3}\sum\limits_{k=1,2,3} L_{GAN}(G,D_k) = \sum\limits_{k=1,2,3}E_{(s,x)}[\log (s,x)] + E_s[\log (Q-D_k(s,G(s)))].
\end{eqnarray}
\label{eqn:adversarial_loss}
Here $Q$ is a patch-wise real/fake vector, based on whether the patch overlaps with the holes. Using multiple GAN discriminators at the same or different image scale has been proposed in unconditional GANs~\cite{durugkar2016generative} and conditional GANs~\cite{wang2017high}. Here we extend the design to take into account inpainting hole locations, which is critical in obtaining semantically and locally coherent image completion results.

Our full objective combines both losses is therefore defined as:
\begin{eqnarray}
\min\limits_G((\max\limits_{D_1, D_2, D_3}\sum\limits_{k=1,2,3} L_{GAN}(G,D_k))+\lambda_{PPL}\sum\limits_{k=1,2}PPL_k(G(s)_p, x_p))
\end{eqnarray}

where $\lambda$ controls the importance of the two terms. We set $\lambda_{PPL}=10$ in our experiments.

\subsection{Procedural Training with Adversarial Loss Annealing}
\label{sec:procedural}
Our experiments show that using the described generator head and training losses already give inpainting results better than state-of-the-art. However, the results can still lack fine details, especially when synthesizing textures. There could also be noise patterns when the image is complicated. A straight-forward effort to improve the results would be to stack more intermediate residual blocks to further expand the receptive view, and also increase the expressiveness of the model. However, we found that directly stacking more residual blocks makes it more difficult to stabilize the training. As the search space becomes much larger, it is also more challenging to find local optimum. In the end, the inpainting quality deteriorates as the model depth increases.

Recently, Progressive GAN~\cite{karras2017progressive} was proposed as a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively, by adding new layers that model increasingly fine details as training progresses. This strategy makes the training faster and more stable and enables it to synthesize mega-pixel images with unprecedented visual quality.  

We adapt this idea for image inpanting and propose to use procedural block-wise training to gradually increase the depth of the inpainting network. More specifically, we begin by training the generator head until it converges. Then, we add a new residual block after the already trained residual blocks, right before the back-end upsampling layers. In order to smoothly introduce the new residual block without suddenly breaking the trained model, we add another skip path from the trained residual blocks to the upsampling layers. Initially, the weight of the skip path is set to 1, while the weight of the path containing the new block is set to 0. This essentially makes the initial network identical to the already trained network. We then slowly decrease the weight of the skip path and increase the weight of the new residual block as training progresses. In this way, the newly introduced residual block is trained to be a fine-tuning component, which adds another layer of fine details to the original results. This step are repeated multiple times, where each time we expand the model by adding a new residual block. In our experiment, we found that the results improve significantly after fine-tuning with the first additional residual block. The output becomes stabilized after three residual blocks, and no discernible changes can be detected if more residual blocks are introduced. The procedural training process is illustrated in Fig..

We observe that the block-wise procedural training has several benefits. First, it guides the training process of a very deep generator. Starting with the generator head and gradually fine-tuning with more residual blocks makes it easier to discover the mapping between the incomplete image and the complete image, even though the search space is large given the diversity of natural images and the random holes. Another benefit is reduced training time, as we found decoupling the training of the generator head and the fine-tuning of additional residual blocks requires significantly less training time comparing with training the network all at once.

\noindent\textbf{Adversarial Loss Annealing} During training, the generator adversarial loss updates the generator weight if the discriminator successfully detects the generated image as fake:
\begin{eqnarray*}
\sum\limits_{k=1,2,3}E_s[\log (\bar{Q}-D_k(s,G(s)))].
\end{eqnarray*}
Note that here $\bar{Q}$ reverses $Q$ of~\ref{eqn:adversarial_loss}. As shown in Figure., we observe that the generator adversarial loss becomes dominant over PPL as training progresses. This is because the discriminator becomes increasingly good at detecting fake images during training. This is less of a problem for image synthesis tasks. However, for our inpainting task, the outcome is that the generator deliberately adds noise patterns to confuse the discriminators, bringing more artifact to output or even synthesizing wrong textures. Based on this observation, we propose to use adversarial loss annealing, which decreases the weight of the generator adversarial loss when adding new residual block. More formally, let the initial weight of the generator adversarial loss be $\lambda^0_{G_{adv}}$, and the weight of the generator adversarial loss be $\lambda^i_{G_{adv}}$ after adding the $i_{th}$ residual block. We found that simply decay the weight linearly by setting $\lambda^i_{G_{adv}}=0.1^i\lambda^0_{G_{adv}}$ reduces the noise level. Detailed results are shown in Sec.~\ref{sec:results}.

We summarize the discussed training schemes in Alg.~\ref{algo}.
\begin{algorithm}
\caption{Training the Inpainting Network}\label{algo}
\begin{algorithmic}[1]
\State Set $\lambda_{PPL}\gets 10$ and $\lambda^0_{G_{adv}}$
\State Train the Generator Head $G_0$ until converges.
\For{i=1 to 3}
\State Set $\lambda^i_{G_{adv}}=0.1^i\lambda^0_{G_{adv}}$
\State Add the skip path and the residual block $r_i$ 
\State Train the new generator $G_i$ until it converges.
\EndFor
\State \textbf{return} $G_3$ 
\end{algorithmic}
\end{algorithm}
