%!TEX root = guided_inpainting_paper.tex
\section{Results}
\label{sec:results}
In this section, we first describe our datasets and experiment setting (Sec.~\ref{exp:setup}). We then provide quantitative and qualitative comparisons with several methods, and also report a subjective human perceptual test based on user-study (Sec.~\ref{exp:comparison}). In Sec.~\ref{exp:study}, we conduct several ablation study about the design choice of the framework. Finally, we show how our method can be applied to image harmonization and interactive guided inpainting (Sec.~\ref{exp:guided}).

\subsection{Experiment Setup}
\label{exp:setup}
We evaluate our inpainting method on ImageNet~\cite{russakovsky2015imagenet}, COCO~\cite{lin2014microsoft}, Place2~\cite{zhou2016places} and CelebA~\cite{liu2015faceattributes}. CelebA consists of 202,599 face images with various viewpoints and expressions. For a fair comparison, we follow the standard split with 162,770 images for training, 19,867 for validation and 19,962 for testing.

In order to compare with existing methods, we train on images of size 256x256. We also train another network at a larger scale of 512x512 to demonstrate its ability to handle higher resolutions. For each image, we apply a mask with a single hole or multiple holes placed at random locations. The size of hole is between 1/4 and 1/2 of the image's size. We set the batch size to 8, and regardless of the actual dataset size, we train 150,000 iterations for the generator head and another 1,500 iterations for each additional residual block. For each dataset, the training takes around 2 days to finish on a single Titan X GPU, which is significantly less time than~\cite{iizuka2017globally}.

\begin{figure*}[!ht]
\centering
\setlength\tabcolsep{1pt}
\begin{tabular}{ccccccc}
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0011_input_image.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0011_pm.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/0011_ce2.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/0011_nps2.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0011_siggraph2017.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0011_g.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0011_synthesized_image.jpg}\\
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0034_input_image.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0034_pm.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/0034_ce2.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/0034_nps2.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0034_siggraph2017.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0034_g.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0034_synthesized_image.jpg}\\
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0165_input_image.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0165_pm.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/0165_ce2.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/0165_nps2.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0165_siggraph2017.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0165_g.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0165_synthesized_image.jpg}\\
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0197_input_image.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0197_pm.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/0197_ce2.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/0197_nps2.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0197_siggraph2017.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0197_g.jpg}&
\includegraphics[width=.14\textwidth]{figures/imagenet/imagenet_0197_synthesized_image.jpg}\\
(a) Input & (b) CAF~\cite{barnes2009patchmatch} & (c) CE~\cite{pathak2016context} & (d) NPS~\cite{yang2017high} & (e) GLI~\cite{iizuka2017globally} & (f) GH & (g) Final \\
\end{tabular}
\caption{Fixed hole result comparisons. GH are results generated by generator head, and Final are results generated with block procedural training. All images have original size 256x256. Zoom in for best viewing quality.}
\label{fig:fixed}
\end{figure*}

\begin{figure*}[!ht]
\centering
\small
\setlength\tabcolsep{1pt}
\begin{tabular}{ccccc}
\includegraphics[width=.2\textwidth]{figures/random/000000153299_ip.jpg}&
\includegraphics[width=.2\textwidth]{figures/random/000000153299_pm.jpg}&
\includegraphics[width=.2\textwidth]{figures/random/000000153299_sig.jpg}&
\includegraphics[width=.2\textwidth]{figures/random/000000153299_g.jpg}&
\includegraphics[width=.2\textwidth]{figures/random/000000153299.jpg}\\
\includegraphics[width=.2\textwidth]{figures/random/000000266409_ip.jpg}&
\includegraphics[width=.2\textwidth]{figures/random/000000266409_pm.jpg}&
\includegraphics[width=.2\textwidth]{figures/random/000000266409_sig.jpg}&
\includegraphics[width=.2\textwidth]{figures/random/000000266409_g.jpg}&
\includegraphics[width=.2\textwidth]{figures/random/000000266409.jpg}\\
\includegraphics[width=.2\textwidth]{figures/random/000000286994_ip.jpg}&
\includegraphics[width=.2\textwidth]{figures/random/000000286994_pm.jpg}&
\includegraphics[width=.2\textwidth]{figures/random/000000286994_sig.jpg}&
\includegraphics[width=.2\textwidth]{figures/random/000000286994_g.jpg}&
\includegraphics[width=.2\textwidth]{figures/random/000000286994.jpg}\\
(a) Input & (b) CAF~\cite{barnes2009patchmatch} & (c) GLI~\cite{iizuka2017globally} & (d) GH & (e) Final \\
\end{tabular}
\caption{Random hole completion comparison. GH are results generated by generator head, and Final are results generated with block procedural training. All images have original size 256x256. Zoom in for best viewing quality.}
\label{fig:random}
\end{figure*}

\subsection{Comparison with Existing Methods}
\label{exp:comparison}
For ImageNet, COCO and Place2, we compare our results with Content-Aware Fill (CAF)~\cite{barnes2009patchmatch}, Context Encoder (CE)~\cite{pathak2016context}, Neural Patch Synthesis (NPS)~\cite{yang2017high} and Global Local Inpainting (GLI)~\cite{iizuka2017globally}. For CE, NPS, and GLI, we use pre-trained models made available by the authors. CE and NPS are only trained with holes with fixed size and location (image center), while CAF and GLI can handle arbitrary holes. For fair comparisons, we evaluate on both settings. For center hole completion, we compare with CAF, CE, NPS and GLI on ImageNet~\cite{russakovsky2015imagenet} test images. For random hole completion, we compare with CAF and GLI using test images from COCO and Place2. Only GLI applies Poisson Blending as post-processing. We also show the results by our \textit{Generator Head} and compare. The images shown in Fig.~\ref{fig:fixed},~\ref{fig:random} are randomly sampled from the test set. 

We see that, while CAF can generate realist-looking details using patch propagation, they often fail to capture the global structure or synthesize new contents. CE's result is significantly blurrier as it can only inpaint low-resolution images. NPS's results heavily depend on CE's initialization. Our approach is much faster and generates results of higher visual quality most of the time. Comparing with GLI, our results do not need post-processing, and are more coherent with less artifacts. Our final results also show significant improvement over Generator Head, demonstrating the effectiveness block-wise procedural fine-tuning.

For CelebA, we compare our results with Generative Face Completion~\cite{li2017generative} in Fig.~\ref{fig:face}. Since~\cite{li2017generative}'s model inpaints images of 128x128, we directly up-sample the results to 256x256 to compare. The images shown are also chosen at random. We can see that while~\cite{li2017generative} is an approach specific for face inpainting, our method for general inpainting tasks actually generates much better face completion results.

\begin{figure*}[!ht]
\centering
\small
\setlength\tabcolsep{1pt}
\begin{tabular}{cccccc}
\includegraphics[width=.16\textwidth]{figures/face/000189_input_image.jpg}&
\includegraphics[width=.16\textwidth]{figures/face/res1/res.jpg}&
\includegraphics[width=.16\textwidth]{figures/face/000189_synthesized_image.jpg}&
\includegraphics[width=.16\textwidth]{figures/face/000194_input_image.jpg}&
\includegraphics[width=.16\textwidth]{figures/face/res2/res.jpg}&
\includegraphics[width=.16\textwidth]{figures/face/000194_synthesized_image.jpg}\\
\includegraphics[width=.16\textwidth]{figures/face/000197_input_image.jpg}&
\includegraphics[width=.16\textwidth]{figures/face/res3/res.jpg}&
\includegraphics[width=.16\textwidth]{figures/face/000197_synthesized_image.jpg}&
\includegraphics[width=.16\textwidth]{figures/face/000200_input_image.jpg}&
\includegraphics[width=.16\textwidth]{figures/face/res4/res.jpg}&
\includegraphics[width=.16\textwidth]{figures/face/000200_synthesized_image.jpg}\\
(a) Input & (b) GFC~\cite{li2017generative} & (c) Ours & (d) Input & (e) GFC~\cite{li2017generative} & (f) Ours \\
\end{tabular}
\caption{Face completion results comparing with~\cite{li2017generative}.}
\label{fig:face}
\vspace{-10pt}
\end{figure*}  

\noindent\textbf{Quantitative Evaluation} Table~\ref{table:numerical} shows quantitative comparison between CAF, CE, GLI and our approach. The values are computed based on a random subset of 200 images selected from the test set. Both $\ell_1$ and $\ell_2$ errors are computed with pixel values normalized between $[0,1]$ and are summed up using all pixels of the image. We can see that our method performs better than other methods in terms of SSIM and $\ell_1$ error. For $\ell_2$, CAF and GLI have smaller errors. This is understandable as our model is trained with perceptual loss rather than $\ell_2$ loss. Besides, $\ell_2$ loss awards averaging color values and does not faithfully reflect perceptual quality. From the numerical values, we can also see that procedural fine-tuning reduces the errors from the generator head. 

\begin{table}[!ht]
\begin{center}
\resizebox{.5\textwidth}{!}{%
{\tiny
  \begin{tabular}{ l  c c c }
    \hline
    \textbf{Method} & \textbf{Mean $\ell_1$ Error} & \textbf{Mean $\ell_2$ Error} & \textbf{SSIM} \\ \hline
    \multirow{2}{*}{CAF~\cite{barnes2009patchmatch}} & 968.8 & \textbf{209.5} & 0.9415\\ \cline{2-4}
    & 1660 & \textbf{363.5} & 0.9010\\ \hline
    \multirow{2}{*}{CE~\cite{pathak2016context}} & 2693 & 545.8 & 0.7719\\ \cline{2-4}
    & N/A & N/A & N/A\\ \hline
    \multirow{2}{*}{GLI~\cite{iizuka2017globally}} & 868.6 & 269.7 & 0.9452 \\ \cline{2-4}
    & 1640 & 378.7 & 0.9020\\ \hline
    \multirow{2}{*}{Ours (Generator Head)} & 913.8 & 245.6 & 0.9458  \\ \cline{2-4}
    & 1629 & 439.4 & 0.9073\\ \hline
    \multirow{2}{*}{Ours (Final)} & \textbf{838.3} & 253.3 & \textbf{0.9486} \\ \cline{2-4}
    & \textbf{1609} & 427.0 & \textbf{0.9090}\\ \hline\hline

  \end{tabular}}
  }
  \end{center}
  \caption{Numerical comparison between CAF, CE and GLI, our generator head results and our final results. Up/down are results of center/random region completion. Note that for SSIM, larger values mean greater similarity in terms of content structure and indicate better performance.}
  \label{table:numerical}
\end{table}

\noindent\textbf{User Study}
To more rigorously evaluate the performance, we conduct a user study based on the random hole results. We asked for feedback from 20 users, by giving each user 30 tuples of images to rank. Each tuple contains results of NPS, GLI, and ours. The user study shows that our method have highest scores, outperforming NPS and GLI by a large margin. Among 600 total comparisons, our results are ranked the best 72.3\% of the time. Our results are overwhelmingly better than NPS, as our results are ranked better 95.8\% of the time. Comparing with GLI, our results are ranked better 73.5\% of the time and are ranked the same 15.5\% of the time. 

\subsection{Ablation Study}
\label{exp:study}

\noindent\textbf{Comparison of Convolutional Layer} Choosing the proper convolutional layer improves the inpainting quality and also reduces the noise. We consider three types of convolutional layers: original, dilated~\cite{yu2015multi} and interpolated~\cite{odena2016deconvolution}. We train three networks under the same setting to specifically test their effects. We observed that using dilation significantly improves the inpainting quality while using interpolated convolution tends to generate over-smoothed results. Fig.~\ref{fig:conv} shows a qualitative comparison. 

\begin{figure*}[!ht]
\centering
\small
\setlength\tabcolsep{1pt}
\begin{tabular}{ccccc}
\includegraphics[width=.2\textwidth]{figures/conv/000000311303_input_image.jpg}&
\includegraphics[width=.2\textwidth]{figures/conv/000000311303_synthesized_image.jpg}&
\includegraphics[width=.2\textwidth]{figures/conv/000000311303_synthesized_image-0.jpg}&
\includegraphics[width=.2\textwidth]{figures/conv/000000311303_synthesized_image-1.jpg}&
\includegraphics[width=.2\textwidth]{figures/conv/000000311303_synthesized_image_final.jpg}\\
(a) & (b) & (c) & (d) & (e)  \\
\end{tabular}
\caption{Visual comparison of a result using different types of convolutional layers. (a) Input; (b) Original Conv; (c) Interpolated Conv; (d) Dilated+Interpolated Conv; (e) Dilated Conv (ours).}
\label{fig:conv}
\end{figure*}  

\noindent\textbf{Effect of Procedural Training} Qualitative comparisons in Sec.~\ref{exp:comparison} shows that procedurally adding residual blocks to fine-tune improves the results. However, one may wonder whether we can directly train a deeper network. To find out, we trained a network with 12 residual blocks from scratch, keeping all other hyper-parameters the same. Fig.~\ref{fig:proc} shows two examples of the result comparisons. We found that further increasing the number of residual blocks has a negative effect on inpainting, possibly due to a variety of factors such as gradient vanishing, optimization difficulty, etc. This demonstrates the necessity and importance of the procedural training scheme.

\begin{figure*}[!ht]
\centering
\small
\setlength\tabcolsep{1pt}
\begin{tabular}{cccccc}
\includegraphics[width=.17\textwidth]{figures/proc/000000241668_input_image.jpg}&
\includegraphics[width=.17\textwidth]{figures/proc/000000241668_synthesized_image-1.jpg}&
\includegraphics[width=.17\textwidth]{figures/proc/000000241668_synthesized_image.jpg}&
\includegraphics[width=.17\textwidth]{figures/proc/000000314034_input_image.jpg}&
\includegraphics[width=.17\textwidth]{figures/proc/000000314034_synthesized_image-1.jpg}&
\includegraphics[width=.17\textwidth]{figures/proc/000000314034_synthesized_image.jpg}\\
(a) Input & (b)  & (c)  & (d) Input & (e)  & (f)  \\
\end{tabular}
\caption{Visual comparison of results by directly training a network of 12 blocks ((b),(e)) and procedural training ((c), (f)).}
\label{fig:proc}
\end{figure*}  

\noindent\textbf{Effect of Adversarial Loss Annealing} We also investigate the effect of adversarial loss annealing and train another network without using ALA. Visually, we observe that using adversarial loss annealing reduces the noise level without sacrificing the overall sharpness and realism. We show two randomly selected examples in Fig.~\ref{fig:ala} to illustrate the effects. 

\begin{figure*}[!ht]
\centering
\small
\setlength\tabcolsep{1pt}
\begin{tabular}{cccccc}
\includegraphics[width=.17\textwidth]{figures/ALA/000000063154_input_image.jpg}&
\includegraphics[width=.17\textwidth]{figures/ALA/000000063154_synthesized_image.jpg}&
\includegraphics[width=.17\textwidth]{figures/ALA/000000063154_synthesized_image-1.jpg}&
\includegraphics[width=.17\textwidth]{figures/ALA/000000475779_input_image.jpg}&
\includegraphics[width=.17\textwidth]{figures/ALA/000000475779_synthesized_image.jpg}&
\includegraphics[width=.17\textwidth]{figures/ALA/000000475779_synthesized_image-1.jpg}\\
(a) Input & (b) w/o ALA & (c) ALA & (d) Input & (e) w/o ALA & (f) ALA \\
\end{tabular}
\caption{Visual comparison of results by training training without and with ALA.}
\label{fig:ala}
\end{figure*}  

\begin{figure*}[h!]  
\centering  
\small  
\setlength\tabcolsep{1pt}
\begin{tabular}{cccccc} 
\includegraphics[width=.17\textwidth]{figures/loss/000000090208_input_image.jpg}& 
\includegraphics[width=.17\textwidth]{figures/loss/000000090208_synthesized_image.jpg}& 
\includegraphics[width=.17\textwidth]{figures/loss/000000090208_synthesized_image_final.jpg}& 
\includegraphics[width=.17\textwidth]{figures/loss/000000490171_input_image.jpg}& 
\includegraphics[width=.17\textwidth]{figures/loss/000000490171_synthesized_image.jpg}& 
\includegraphics[width=.17\textwidth]{figures/loss/000000490171_synthesized_image_final.jpg}\\
(a) Input & (b) $\ell_2$ & (c) Ours & (d) Input & (e) $\ell_2$ & (f) Ours\\ 
\end{tabular} 
\caption{Effects of different types of reconstruction losses. Zoom in for best quality.}
\label{fig:ppl} 
\end{figure*}  


\noindent\textbf{Comparison of Patch Perceptual Losses and $\ell_2$} We compare the quality of inpainting that is trained with Patch Perceptual Loss and $\ell_2$ loss. $\ell_2$ is used to train CE and GLI, but it does not correspond well to human perception of similarity. We investigate how our Patch Perceptual Loss compares with $\ell_2$ loss by training under the same condition but using different reconstruction losses (PPL vs $\ell_2$). From the test results, we see that our results are overwhelmingly better than $\ell_2$ results in terms of sharpness and coherence with context. Fig.~\ref{fig:ppl} shows two examples of comparison. 

\begin{figure*}[h!]  
\centering  
\small  
\setlength\tabcolsep{2pt}
\begin{tabular}{cc} 
\includegraphics[width=.45\textwidth]{figures/random_512/test_latest/images/000000565778_input_image.jpg}&
\includegraphics[width=.45\textwidth]{figures/random_512/test_latest/images/000000565778_synthesized_image.jpg}\\ 
\includegraphics[width=.45\textwidth]{figures/random_512/test_latest/images/000000344611_input_image.jpg}&
\includegraphics[width=.45\textwidth]{figures/random_512/test_latest/images/000000344611_synthesized_image.jpg}\\ 
(a) 512x512 Input & (b) Our Inpainting Result \\ 
\end{tabular} 
\caption{Inpainting results of 512x512 images.}
\label{fig:large} 
\end{figure*}  

\noindent\textbf{Results on Large Images} To showcase our ability to handle large images and holes, we show two inpainting results on 512x512 images in Fig.~\ref{fig:large}. Given the size of the receptive field and the depth of the network, our approach is able to inpaint visually plausible and semantically meaningful contents in these high-resolution images. 

\begin{figure*}[!ht]
\centering
\small
\setlength\tabcolsep{1pt}
\begin{tabular}{cccccc}
  \includegraphics[width=.17\textwidth]{figures/hm/000000319696_input_image.jpg}&
  \includegraphics[width=.17\textwidth]{figures/hm/000000319696_synthesized_image.jpg}&
  \includegraphics[width=.17\textwidth]{figures/hm/000000319696_synthesized_image-1.jpg}&
  \includegraphics[width=.17\textwidth]{figures/hm/000000159311_input_image.jpg}&
  \includegraphics[width=.17\textwidth]{figures/hm/000000159311_synthesized_image.jpg}&
  \includegraphics[width=.17\textwidth]{figures/hm/000000159311_synthesized_image-1.jpg} \\
  (a) Input & (b) DH~\cite{tsai2017deep}  & (c) Ours & (d) Input & (e) DH~\cite{tsai2017deep}  & (f) Ours \\
\end{tabular}
\caption{Examples of image harmonization results. For (a) and (d), the microwave and the zebra on the back have unusual color. Our method correctly adjusts their appearance and makes the images coherent and realistic. }
\label{fig:harm}
\end{figure*}

\begin{figure*}[!ht]
\centering
\small
\setlength\tabcolsep{1pt}
\begin{tabular}{cccc}
  \includegraphics[width=.25\textwidth]{figures/guided/000000026204_input.jpg}&
  \includegraphics[width=.25\textwidth]{figures/guided/000000026204_mask.jpg}&
  \includegraphics[width=.25\textwidth]{figures/guided/000000026204_inpainting.jpg}&
  \includegraphics[width=.25\textwidth]{figures/guided/000000026204_inpainting_harmonization.jpg} \\
  \includegraphics[width=.25\textwidth]{figures/guided/000000488251_input.jpg}&
  \includegraphics[width=.25\textwidth]{figures/guided/000000488251_mask.jpg}&
  \includegraphics[width=.25\textwidth]{figures/guided/000000488251_inpainting.jpg}&
  \includegraphics[width=.25\textwidth]{figures/guided/000000488251_inpainting_harmonization.jpg} \\
  \includegraphics[width=.25\textwidth]{figures/gi/images/000000190007_input.jpg}&
\includegraphics[width=.25\textwidth]{figures/gi/images/000000190007_mask.jpg}&
\includegraphics[width=.25\textwidth]{figures/gi/images/000000190007_inpainting.jpg}&
\includegraphics[width=.25\textwidth]{figures/gi/images/000000190007_inpainting_harmonization.jpg}\\ 
\includegraphics[width=.25\textwidth]{figures/gi/images/000000355905_input.jpg}&
\includegraphics[width=.25\textwidth]{figures/gi/images/000000355905_mask.jpg}&
\includegraphics[width=.25\textwidth]{figures/gi/images/000000355905_inpainting.jpg}&
\includegraphics[width=.25\textwidth]{figures/gi/images/000000355905_inpainting_harmonization.jpg}\\ 
  (a) Input & (b) Segmentation  & (c) Inpainting & (d) Final result  \\
\end{tabular}
\caption{Examples of interactive guided inpainting result. The segmentation mask is given by our foreground/background segmentation network trained on COCO. The final result combines the outputs of harmonization and inpainting.}
\label{fig:guided}
\end{figure*}


\subsection{Interactive Guided Inpainting}
\label{exp:guided} 

In this section, we consider object-based guided inpainting as a practical scenario. Specifically, we assume the user would like to add to the input image with objects from another guide image. Given an input image $I$ and a guide image $I_g$, we allow the user to select a region of $I_s$ by dragging a bounding box $B_g$ containing the object that he desires to add. Note that unlike previous settings of image composition, we do not require the user to accurately segment the object, but instead only providing a bounding box is sufficient. This greatly reduces the workload and simplifies the task. Next, we perform segmentation use a network to extract the object foreground. The segmentation network is adapted from Mask R-CNN~\cite{he2017mask} and trained on COCO, but is agnostic to object categories and only considers the foreground/background classification. Finally, we resize and paste $B_g$ to $I$. To make the composition look natural and realistic, we need to remove and inpaint the background of $B_g$ and also perform harmonization on the foreground object such that it is coherent the overall image appearance.   

To accomplish this task with known segmentation, we need to address two separate problems: inpainting and harmonization. Our model can be easily extended to train for both tasks at the same time, only requiring changing the input and output. To train this model, the data acquisition is similar to~\cite{tsai2017deep}, where we create artificially composited images using statistics color transfer~\cite{reinhard2001color} and another randomly chosen guide image. More specifically, at each iteration given the input image $I$, we randomly select another image $I_g$ from the dataset. We then select and segment an object $I_o$ from $I$ based on the segmentation mask of COCO. We then transfer the color from $I_g$ to $I_o$, and paste $I_o$ onto $I_g$ at a random location. Finally, we crop a bounding box $I_b$ from $I_g$ that contains $I_o$, and paste $I_b$ back to $I$. The result, together with the foreground/background segmentation mask of $I_b$, is given to the network as input. The model is modified to output two images, one for inpainting result and the other for harmonization result. We use the original $I$ as the ground truth for both tasks. 

The network jointly trained for inpainting and harmonization performs well in both tasks. Fig.~\ref{fig:harm} shows that it generates harmonization results better than~\cite{tsai2017deep}, which is the state-of-the-art deep harmonization method. Finally, in Fig.~\ref{fig:guided} we show several examples of interactive guided inpainting results.

