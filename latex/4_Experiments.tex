%!TEX root = guided_inpainting_paper.tex
\section{Results}
\label{sec:results}
In this section, we first describe our dataset and experiment setting (Sec.~\ref{exp:setup}). We then provide quantitative and qualitative comparisons with several methods, and also report a subjective human perceptual test with user-study (Sec.~\ref{exp:comparison}). In Sec.~\ref{exp:study}, we conduct several ablation study about the design choice of the models, losses, and training scheme. Finally, we show how our method can be applied to real use cases of object removal and guided inpanting. In particular, the inpainting model can be adapted to train an image harmonization network, which generates state-of-the-art harmonization results. We then demonstrate that by jointly training a model for inpainting and harmonization we can easily achieve guided inpainting (Sec.~\ref{exp:guided}).

\subsection{Experiment Setup}
\label{exp:setup}
We evaluate our inpainting method on several representative datasets. COCO~\cite{lin2014microsoft} is a large dataset containing both object and scene images; Place2~\cite{zhou2016places} includes images of a diversity of scenes and was originally meant for scene classification; finally, we train and test on CelebA~\cite{liu2015faceattributes}, which consists of 202,599 face images with various viewpoints and expressions. For a fair comparison, we follow the standard split with 162,770 images for training, 19,867 for validation and 19,962 for testing.

In order to compare with existing methods, we train on images of size 256x256. We also train another network at a larger scale of 512x512 to demonstrate its ability to handle higher resolutions. As the pre-processing step, we first resize the image and then conduct random cropping. We then apply data augmentation with random flipping. For each image, we create a mask containing one or two rectangle holes. The size of the hole ranges from 1/4 to 1/2 of the image's dimension, and are positioned at random locations. Note that during inference, our network is able to handle masks with arbitrary number of holes of any shape. Finally, we shift and rescale the pixel value from [0,255] to [-1,1] and fill in the masked regions with zeros. We then concatenate the corrupted image and the mask as input.

For all our training, we set the learning rate with polynomial decay starting from 0.0002, and adopt Adam for optimization. We set the batch size to 8, and regardless of the actual dataset size, we train 150,000 iterations for the generator head and another 1,500 iterations for each additional residual block. For each dataset, the training takes around 2 days to finish on a single Titan X GPU. 

\subsection{Comparison with Existing Methods}
\label{exp:comparison}
We compare our results with Content-Aware Fill (CAF)~\cite{barnes2009patchmatch}, Context Encoder (CE)~\cite{pathak2016context} and Global-Local Inpainting (GLI)~\cite{iizuka2017globally}. For CE and GLI, we use off-the-shelf pre-trained model from the web. Given CE's model is trained with square holes located at the center, we evaluate on both settings of arbitrary holes (Fig. ) and center hole (Fig.). For center hole completion, we compare with CAF, CE and GLI on ImageNet test images~\cite{russakovsky2015imagenet}, where our results are directly generated by models trained on COCO. For arbitrary shape completion, we compare with CAF and GLI using images from COCO, Place2 and CelebA. As GLI applies Poisson Blending for post-processing, we show both their results before and after post-processing. For our approach, we show the results generated by the Generator Head alone as well as the final results using procedural training as refinement. The comparison results shown are randomly sampled from the entire test set.

Based on the visual results we can see that, for CAF as a non-learning and patch-based approach, its main issue is the inability to generate novel objects not available in the known context. This is especially an issue for highly specific and complex structure such as face inpainting. Furthermore, while CAF is able to generate realistic-looking details, they do not always capture the global structure and the inpainting is often inconsistent when the contexts are complex. CE's result is blurrier, and the border between the hole and the context is easily detectable. Comparing our approach with GLI, even without fine-tuning, our results exhibit better textures and less noise, and are visually more coherent with the surrounding context. This is especially true for large holes. 

\noindent\textbf{Quantitative Evaluation} Table. shows quantitative comparison between CAF, CE, GLI and our approach. The values are computed based a random subset of 200 images selected from the test set. We can see that our method performs better than other methods. In addition, our procedural training further reduces the losses from the generator head.

\noindent\textbf{User study}


\subsection{Ablation Study}
\label{exp:study}

\noindent\textbf{Failure Cases}
\subsection{A user interface of guided inpainting}
\label{exp:guided} 

For guided inpainting, the data acquistion is a bit two different. We randomly crop a bounding box containing the object from the original image, and we then keep the object only and discard the background. We then randomly select another image from dataset, and do color transfer using []'s algorithm. After that we paste the object onto the randomly selected image and then crops a bounding box, and paste it back to the original image. Assuming we know the segmentation, we have two masks as input, one is the background inside the bounding box for inpainting, and the other image the object mask for harmonization. We use the same network and losses as unguided inpaiting, only this time we output two images, one for the inpainting result, and the other for harmonization result. The final result is a composition of the inpainting result and harmonization result.

We use a  
 paste the object (without the hole) onto another randomly select it object object it into another image. We then use color transfer to transfer the color of the object. 


COCO (?) and Place2 and CelebA dataset (?). COCO contains ? training images and ? test images. Place2 contains ?  and ? test images. The place
We train and test on four datasets, the COCO, ADE20K, CelebA, and Place. 

For guided inpainting, we trained on COCO dataset. Our data acquisation is as following. The input is an image a.

Figure a.



